# -*- coding: utf-8 -*-
"""Human pose estimation with MoveNet

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/human-pose-estimation-with-movenet-c93f4727-cde8-40e0-8245-8fdfde335e10.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20241114/auto/storage/goog4_request%26X-Goog-Date%3D20241114T043342Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D1bfe4287e715587c50428e70bf3b6a73c6a2a4695081b64db1026e259340182c738f3f77718e1a313f45acdcf9fe777a05b2e55142d46adac259365de7b0c409c37d586b1388f6a165e67846813a2479da4a3639d1f28b1343c1390bd583ae8fee511b445b3baf6c4d4d49eec2c62ac370274ff955b6045b979f0eb12b041391f8e6eca2c048d684414affbe9ad703fd56ffb40272eaf77ca4ffa44768997ebc307f61e5f519429ae5a2d37a408efaaf948102bed7e39d25821b6337032d467d1cdfa20dd296046b398ae2a0d5a0e2c920e45e936595ffd673e2673e363a90268aa7c295b4af31b93149038bf572eb3430c9da45782421b8b8f03449a4652132

# Introduction

In this tutorial, we will make use of the next-generation Pose Detection model from Google Research which can detect up to 17 keypoints in the human body.

# MoveNet architecture in a nutshell

## How does it work ?

MoveNet uses heatmaps to accurately localize human keypoints. It's a **bottom-up** estimation model, which means that it first detects the human joints of all persons, and then assemble these joints into poses for each person.[[Source]](https://arxiv.org/pdf/1807.09972.pdf#:~:text=The%20top%2Ddown%20approaches%20first,full%20poses%20for%20all%20persons.)

## Architecture (2 main components)

- **Feature extractor** : A MobileNetV2 with an attached feature pyramid network
- **A set of predictor heads** : attached to the feature extractor. They are responsible for predicting :
 - the geometric center of the instances (persons)
 - the full set of keypoints for a person
 - the location of all the keypoints
 - local offsets from each output feature map pixel to the precise sub-pixel location of each keypoint


## [A deeper explanation on the MoveNet processing steps](https://blog.tensorflow.org/2021/05/next-generation-pose-detection-with-movenet-and-tensorflowjs.html). Now, let's start coding !

# Libraries

> *Notes* :
> - **LineCollection** : provides us with some tools to draw lines.
> - **Matplotlib patches** : patches module in matplotlib allows us add to shapes like rectangles, polygons...on top of a plot.
> - **Imageio** : library that provides an easy interface to read and write a wide range of image data, including animated images. Will be helpful in our case espacially when writing the new gif which includes the keypoints.
"""

import tensorflow as tf
import tensorflow_hub as hub
import numpy as np
import cv2
import imageio

import matplotlib.pyplot as plt
import matplotlib.patches as patches

from IPython.display import HTML, display
from matplotlib.collections import LineCollection

"""# Setup

## Map the joint names to keypoint indices
"""

KEYPOINT_DICT = {
    'nose': 0,
    'left_eye': 1,
    'right_eye': 2,
    'left_ear': 3,
    'right_ear': 4,
    'left_shoulder': 5,
    'right_shoulder': 6,
    'left_elbow': 7,
    'right_elbow': 8,
    'left_wrist': 9,
    'right_wrist': 10,
    'left_hip': 11,
    'right_hip': 12,
    'left_knee': 13,
    'right_knee': 14,
    'left_ankle': 15,
    'right_ankle': 16
}

"""## Map the bones (keypoint edges) to a matplotlib color name

 Reference : [Matplotlib colors](https://matplotlib.org/stable/gallery/color/named_colors.html)

> ![Colors_index](https://raw.githubusercontent.com/Justsecret123/Human-pose-estimation/main/Screenshots/mpl_colors.PNG)
"""

KEYPOINT_EDGE_INDS_TO_COLOR = {
    (0, 1): 'm',
    (0, 2): 'c',
    (1, 3): 'm',
    (2, 4): 'c',
    (0, 5): 'm',
    (0, 6): 'c',
    (5, 7): 'm',
    (7, 9): 'm',
    (6, 8): 'c',
    (8, 10): 'c',
    (5, 6): 'y',
    (5, 11): 'm',
    (6, 12): 'c',
    (11, 12): 'y',
    (11, 13): 'm',
    (13, 15): 'm',
    (12, 14): 'c',
    (14, 16): 'c'
}

"""## Helper functions"""

def _keypoints_and_edges_for_display(
    keypoints_with_scores,
    height,
    width,
    keypoint_threshold=0.11,
    ):
    """Returns high confidence keypoints and edges for visualization.

  Args:
    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing
      the keypoint coordinates and scores returned from the MoveNet model.
    height: height of the image in pixels.
    width: width of the image in pixels.
    keypoint_threshold: minimum confidence score for a keypoint to be
      visualized.

  Returns:
    A (keypoints_xy, edges_xy, edge_colors) containing:
      * the coordinates of all keypoints of all detected entities;
      * the coordinates of all skeleton edges of all detected entities;
      * the colors in which the edges should be plotted.
  """

    keypoints_all = []
    keypoint_edges_all = []
    edge_colors = []
    (num_instances, _, _, _) = keypoints_with_scores.shape
    for idx in range(num_instances):
        kpts_x = keypoints_with_scores[0, idx, :, 1]
        kpts_y = keypoints_with_scores[0, idx, :, 0]
        kpts_scores = keypoints_with_scores[0, idx, :, 2]
        kpts_absolute_xy = np.stack([width * np.array(kpts_x), height
                                    * np.array(kpts_y)], axis=-1)
        kpts_above_thresh_absolute = kpts_absolute_xy[kpts_scores
                > keypoint_threshold, :]
        keypoints_all.append(kpts_above_thresh_absolute)

        for (edge_pair, color) in KEYPOINT_EDGE_INDS_TO_COLOR.items():
            if kpts_scores[edge_pair[0]] > keypoint_threshold \
                and kpts_scores[edge_pair[1]] > keypoint_threshold:
                x_start = kpts_absolute_xy[edge_pair[0], 0]
                y_start = kpts_absolute_xy[edge_pair[0], 1]
                x_end = kpts_absolute_xy[edge_pair[1], 0]
                y_end = kpts_absolute_xy[edge_pair[1], 1]
                line_seg = np.array([[x_start, y_start], [x_end,
                                    y_end]])
                keypoint_edges_all.append(line_seg)
                edge_colors.append(color)
    if keypoints_all:
        keypoints_xy = np.concatenate(keypoints_all, axis=0)
    else:
        keypoints_xy = np.zeros((0, 17, 2))

    if keypoint_edges_all:
        edges_xy = np.stack(keypoint_edges_all, axis=0)
    else:
        edges_xy = np.zeros((0, 2, 2))
    return (keypoints_xy, edges_xy, edge_colors)


def draw_prediction_on_image(
    image,
    keypoints_with_scores,
    crop_region=None,
    close_figure=False,
    output_image_height=None,
    ):
    """Draws the keypoint predictions on image.

  Args:
    image: A numpy array with shape [height, width, channel] representing the
      pixel values of the input image.
    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing
      the keypoint coordinates and scores returned from the MoveNet model.
    crop_region: A dictionary that defines the coordinates of the bounding box
      of the crop region in normalized coordinates (see the init_crop_region
      function below for more detail). If provided, this function will also
      draw the bounding box on the image.
    output_image_height: An integer indicating the height of the output image.
      Note that the image aspect ratio will be the same as the input image.

  Returns:
    A numpy array with shape [out_height, out_width, channel] representing the
    image overlaid with keypoint predictions.
  """

    (height, width, channel) = image.shape
    aspect_ratio = float(width) / height
    (fig, ax) = plt.subplots(figsize=(12 * aspect_ratio, 12))

  # To remove the huge white borders

    fig.tight_layout(pad=0)
    ax.margins(0)
    ax.set_yticklabels([])
    ax.set_xticklabels([])
    plt.axis('off')

    im = ax.imshow(image)
    line_segments = LineCollection([], linewidths=4, linestyle='solid')
    ax.add_collection(line_segments)

  # Turn off tick labels

    scat = ax.scatter([], [], s=60, color='#FF1493', zorder=3)

    (keypoint_locs, keypoint_edges, edge_colors) = \
        _keypoints_and_edges_for_display(keypoints_with_scores, height,
            width)

    line_segments.set_segments(keypoint_edges)
    line_segments.set_color(edge_colors)
    if keypoint_edges.shape[0]:
        line_segments.set_segments(keypoint_edges)
        line_segments.set_color(edge_colors)
    if keypoint_locs.shape[0]:
        scat.set_offsets(keypoint_locs)

    if crop_region is not None:
        xmin = max(crop_region['x_min'] * width, 0.0)
        ymin = max(crop_region['y_min'] * height, 0.0)
        rec_width = min(crop_region['x_max'], 0.99) * width - xmin
        rec_height = min(crop_region['y_max'], 0.99) * height - ymin
        rect = patches.Rectangle(
            (xmin, ymin),
            rec_width,
            rec_height,
            linewidth=1,
            edgecolor='b',
            facecolor='none',
            )
        ax.add_patch(rect)

    fig.canvas.draw()
    image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(),
                                    dtype=np.uint8)
    image_from_plot = \
        image_from_plot.reshape(fig.canvas.get_width_height()[::-1]
                                + (3, ))
    plt.close(fig)
    if output_image_height is not None:
        output_image_width = int(output_image_height / height * width)
        image_from_plot = cv2.resize(image_from_plot,
                dsize=(output_image_width, output_image_height),
                interpolation=cv2.INTER_CUBIC)
    return image_from_plot


def to_gif(images, fps):
    """Converts image sequence (4D numpy array) to gif."""

    imageio.mimsave('./animation.gif', images, fps=fps)
    return embed.embed_file('./animation.gif')


def progress(value, max=100):
    return HTML("""
      <progress
          value='{value}'
          max='{max}',
          style='width: 100%'
      >
          {value}
      </progress>
  """.format(value=value,
                max=max))

"""## Cropping algorithm, as provided by [the official tutorial](https://www.tensorflow.org/hub/tutorials/movenet#helper_functions_for_visualization)

> *Note* :
> The cropping algorithm applies intelligent cropping based on detection from previous frames, thus allowing the model to focus on the main subject. Therefore, much better precision is obtained without sacrificing the speed.
"""

# Confidence score to determine whether a keypoint prediction is reliable.
MIN_CROP_KEYPOINT_SCORE = 0.2

def init_crop_region(image_height, image_width):
    """Defines the default crop region.

  The function provides the initial crop region (pads the full image from both
  sides to make it a square image) when the algorithm cannot reliably determine
  the crop region from the previous frame.
  """

    if image_width > image_height:
        box_height = image_width / image_height
        box_width = 1.0
        y_min = (image_height / 2 - image_width / 2) / image_height
        x_min = 0.0
    else:
        box_height = 1.0
        box_width = image_height / image_width
        y_min = 0.0
        x_min = (image_width / 2 - image_height / 2) / image_width

    return {
        'y_min': y_min,
        'x_min': x_min,
        'y_max': y_min + box_height,
        'x_max': x_min + box_width,
        'height': box_height,
        'width': box_width,
        }


def torso_visible(keypoints):
    """Checks whether there are enough torso keypoints.

  This function checks whether the model is confident at predicting one of the
  shoulders/hips which is required to determine a good crop region.
  """

    return (keypoints[0, 0, KEYPOINT_DICT['left_hip'], 2]
            > MIN_CROP_KEYPOINT_SCORE or keypoints[0, 0,
            KEYPOINT_DICT['right_hip'], 2] > MIN_CROP_KEYPOINT_SCORE) \
        and (keypoints[0, 0, KEYPOINT_DICT['left_shoulder'], 2]
             > MIN_CROP_KEYPOINT_SCORE or keypoints[0, 0,
             KEYPOINT_DICT['right_shoulder'], 2]
             > MIN_CROP_KEYPOINT_SCORE)


def determine_torso_and_body_range(
    keypoints,
    target_keypoints,
    center_y,
    center_x,
    ):
    """Calculates the maximum distance from each keypoints to the center location.

  The function returns the maximum distances from the two sets of keypoints:
  full 17 keypoints and 4 torso keypoints. The returned information will be
  used to determine the crop size. See determineCropRegion for more detail.
  """

    torso_joints = ['left_shoulder', 'right_shoulder', 'left_hip',
                    'right_hip']
    max_torso_yrange = 0.0
    max_torso_xrange = 0.0
    for joint in torso_joints:
        dist_y = abs(center_y - target_keypoints[joint][0])
        dist_x = abs(center_x - target_keypoints[joint][1])
        if dist_y > max_torso_yrange:
            max_torso_yrange = dist_y
        if dist_x > max_torso_xrange:
            max_torso_xrange = dist_x

    max_body_yrange = 0.0
    max_body_xrange = 0.0
    for joint in KEYPOINT_DICT.keys():
        if keypoints[0, 0, KEYPOINT_DICT[joint], 2] \
            < MIN_CROP_KEYPOINT_SCORE:
            continue
        dist_y = abs(center_y - target_keypoints[joint][0])
        dist_x = abs(center_x - target_keypoints[joint][1])
        if dist_y > max_body_yrange:
            max_body_yrange = dist_y

        if dist_x > max_body_xrange:
            max_body_xrange = dist_x

    return [max_torso_yrange, max_torso_xrange, max_body_yrange,
            max_body_xrange]


def determine_crop_region(keypoints, image_height, image_width):
    """Determines the region to crop the image for the model to run inference on.

  The algorithm uses the detected joints from the previous frame to estimate
  the square region that encloses the full body of the target person and
  centers at the midpoint of two hip joints. The crop size is determined by
  the distances between each joints and the center point.
  When the model is not confident with the four torso joint predictions, the
  function returns a default crop which is the full image padded to square.
  """

    target_keypoints = {}
    for joint in KEYPOINT_DICT.keys():
        target_keypoints[joint] = [keypoints[0, 0,
                                   KEYPOINT_DICT[joint], 0]
                                   * image_height, keypoints[0, 0,
                                   KEYPOINT_DICT[joint], 1]
                                   * image_width]

    if torso_visible(keypoints):
        center_y = (target_keypoints['left_hip'][0]
                    + target_keypoints['right_hip'][0]) / 2
        center_x = (target_keypoints['left_hip'][1]
                    + target_keypoints['right_hip'][1]) / 2

        (max_torso_yrange, max_torso_xrange, max_body_yrange,
         max_body_xrange) = determine_torso_and_body_range(keypoints,
                target_keypoints, center_y, center_x)

        crop_length_half = np.amax([max_torso_xrange * 1.9,
                                   max_torso_yrange * 1.9,
                                   max_body_yrange * 1.2,
                                   max_body_xrange * 1.2])

        tmp = np.array([center_x, image_width - center_x, center_y,
                       image_height - center_y])
        crop_length_half = np.amin([crop_length_half, np.amax(tmp)])

        crop_corner = [center_y - crop_length_half, center_x
                       - crop_length_half]

        if crop_length_half > max(image_width, image_height) / 2:
            return init_crop_region(image_height, image_width)
        else:
            crop_length = crop_length_half * 2
            return {
                'y_min': crop_corner[0] / image_height,
                'x_min': crop_corner[1] / image_width,
                'y_max': (crop_corner[0] + crop_length) / image_height,
                'x_max': (crop_corner[1] + crop_length) / image_width,
                'height': (crop_corner[0] + crop_length) / image_height \
                    - crop_corner[0] / image_height,
                'width': (crop_corner[1] + crop_length) / image_width \
                    - crop_corner[1] / image_width,
                }
    else:
        return init_crop_region(image_height, image_width)


def crop_and_resize(image, crop_region, crop_size):
    """Crops and resize the image to prepare for the model input."""

    boxes = [[crop_region['y_min'], crop_region['x_min'],
             crop_region['y_max'], crop_region['x_max']]]
    output_image = tf.image.crop_and_resize(image, box_indices=[0],
            boxes=boxes, crop_size=crop_size)
    return tf.cast(output_image, tf.int32)

"""# Load the model from TF hub

> *Notes* : MoveNet is offered with two variants, known as :
> - Lightning : intended for latency-critical applications
> - Thunder : for applications applications that require high accuracy

We will load the multipose *Lightning* model, which is able to detect mutliple people (up to 6 instances) in the image frame at the same time.
"""

model = hub.load("https://tfhub.dev/google/movenet/multipose/lightning/1")
movenet = model.signatures['serving_default']

"""# Adjust the input size

According to [the official documentation](https://tfhub.dev/google/movenet/multipose/lightning/1) :

1. The height/width are both multiple of 32.
2. The height to width ratio is close (and enough) to cover the original image's aspect ratio.
3. Make the larger side to be 256 (one should adjust this based on the speed/accuracy requirements). For example, a 720p image (i.e. 720x1280 (HxW)) should be resized and padded to 160x256 image.
"""

input_size = 256

"""# Inference

## Download the test gif
"""

! wget -O ngannou.gif https://raw.githubusercontent.com/Justsecret123/Human-pose-estimation/main/Test%20gifs/Ngannou_takedown.gif

"""## Load and decode the gif"""

image_path = "./ngannou.gif"
raw_image = tf.io.read_file(image_path)
decoded_image = tf.image.decode_gif(raw_image)
image = tf.cast(decoded_image, dtype=tf.int32)

(num_frames, image_height, image_width, _) = image.shape
crop_region = init_crop_region(image_height, image_width)

"""## Display metadata"""

shape = image.shape
print(f"\n\
      - The gif is divided in {shape[0]} frames\n\
      - Image size : ({shape[1]},{shape[2]})\n\
      - Channels : {shape[3]}\n\
      - Data type : {image.dtype}"
     )

"""## Redefine the inference function"""

def run_inference(
    movenet,
    image,
    crop_region,
    crop_size,
    ):
    """Runs model inference on the cropped region.

  The function runs the model inference on the cropped region and updates the
  model output to the original image coordinate system.
  """

    (image_height, image_width, _) = image.shape
    input_image = crop_and_resize(tf.expand_dims(image, axis=0),
                                  crop_region, crop_size=crop_size)


    #Run model inference.
    output = movenet(input_image)["output_0"]
    keypoints_with_scores = output.numpy()

    max_instances = 6

    #Update the coordinates.
    for idx in range(17):
        keypoints_with_scores[0, 0, idx, 0] = (
            crop_region['y_min'] * image_height +
            crop_region['height'] * image_height *
            keypoints_with_scores[0, 0, idx, 0]) / image_height
        keypoints_with_scores[0, 0, idx, 1] = (
            crop_region['x_min'] * image_width +
            crop_region['width'] * image_width *
            keypoints_with_scores[0, 0, idx, 1]) / image_width
    return keypoints_with_scores

output_images = []
bar = display(progress(0, num_frames - 1), display_id=True)
#For each frame within the gif
for frame_idx in range(num_frames):
    # Run the infereence
    keypoints_with_scores = run_inference(movenet, image[frame_idx, :, :, :], crop_region, crop_size=[input_size, input_size])
    #Append the drawings to the output images (write a frame to the gif)
    output_images.append(draw_prediction_on_image(image[frame_idx, :, : , :].numpy().astype(np.int32),
                         keypoints_with_scores, crop_region=None,
                         close_figure=True, output_image_height=300))
    #Apply the cropping algorithm
    crop_region = determine_crop_region(keypoints_with_scores,
            image_height, image_width)
    #Update the progress bar
    bar.update(progress(frame_idx, num_frames - 1))

"""## Run inference

# Visualize the results
"""

output = np.stack(output_images, axis=0)
to_gif(output, fps=10)

